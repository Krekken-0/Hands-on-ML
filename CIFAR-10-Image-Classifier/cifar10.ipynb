{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c56a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a358de95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf610a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "toTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale = True)])\n",
    "train_valid_dataset = torchvision.datasets.CIFAR10(root = \"./dataset/\", train = True, download=True, transform=toTensor)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = \"./dataset/\", train=False, download=True, transform=toTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c974e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = torch.utils.data.random_split(train_valid_dataset, [45_000, 5_000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bf4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, batch_size = batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a36de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_he_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ab0bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_model(n_hidden, n_neurons, n_inputs, n_outputs):\n",
    "    layers = [nn.Flatten(), nn.Linear(n_inputs, n_neurons),nn.BatchNorm1d(n_neurons), nn.SiLU()]\n",
    "    for _ in range(n_hidden - 1):\n",
    "        layers += [nn.Linear(n_neurons, n_neurons),nn.BatchNorm1d(n_neurons), nn.SiLU()]\n",
    "    layers += [nn.Linear(n_neurons, n_outputs)]\n",
    "    model = nn.Sequential(*layers)\n",
    "    model.apply(use_he_init)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4d52632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_deep_model(20, 100, 3*32*32, 10).to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr = 0.125)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4d1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tm(model: nn.Module, data_loader: DataLoader, metric: torchmetrics.Accuracy):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.inference_mode():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "536619ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bba4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(model: nn.Module, optimizer: torch.optim, loss_fn, metric: torchmetrics.Accuracy,\n",
    "                train_loader: DataLoader, valid_loader: DataLoader, n_epochs: int, patience: int = 10,\n",
    "                checkpoint_path: str = None, scheduler = None):\n",
    "    checkpoint_path = checkpoint_path or \"cifar-10.pt\"\n",
    "    history = {\"train_losses\":[],\n",
    "               \"train_metrics\": [],\n",
    "               \"valid_metrics\": []}\n",
    "    best_metric = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            # print(f\"------Batch statistics-----\", f\"Loss: {loss.item()}\", f\"Total Loss: {total_loss}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "\n",
    "        train_metric = metric.compute().item()\n",
    "        valid_metric = evaluate_tm(model=model, data_loader=valid_loader, metric=metric).item()\n",
    "        if valid_metric > best_metric:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            best_metric = valid_metric\n",
    "            best = \" (best)\"\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            best = \"\"\n",
    "\n",
    "        t1 = time.time()\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        history[\"valid_metrics\"].append(valid_metric)\n",
    "        print(f\"Epoch: {epoch + 1}/{n_epochs}\",\n",
    "              f\"Train Loss: {history[\"train_losses\"][-1]:.5f}\",\n",
    "              f\"Train Metrics: {history[\"train_metrics\"][-1]:.4f}\",\n",
    "              f\"Valid Metrics: {history[\"valid_metrics\"][-1]:.4f}{best} in {t1 - t0:.1f}s\")\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc1e9489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 2.13337 Train Metrics: 0.1780 Valid Metrics: 0.1982 (best) in 6.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m n_epochs = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs, patience, checkpoint_path, scheduler)\u001b[39m\n\u001b[32m     22\u001b[39m total_loss += loss.item()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# print(f\"------Batch statistics-----\", f\"Loss: {loss.item()}\", f\"Total Loss: {total_loss}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m optimizer.step()\n\u001b[32m     27\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "history = train_model(model, optimizer, loss_fn, accuracy, train_loader,\n",
    "                      valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b560e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardize(nn.Module):\n",
    "    def __init__(self, sample):\n",
    "        super().__init__()\n",
    "        flat = torch.flatten(sample, start_dim = 1)\n",
    "        mean = flat.mean(dim = 0, keepdim=True)\n",
    "        std = flat.std(dim = 0, keepdim=True)\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"std\", std)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return (X - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7e7d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = torch.stack([img for img,_ in train_set])\n",
    "standardize = Standardize(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45570e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_lecun_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "465dce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_model_with_selu(n_hidden, n_neurons, n_inputs, n_outputs):\n",
    "    layers = [nn.Flatten(), standardize, nn.Linear(n_inputs, n_neurons), nn.SELU()]\n",
    "    for _ in range(n_hidden - 1):\n",
    "        layers += [nn.Linear(n_neurons, n_neurons), nn.SELU()]\n",
    "    layers += [nn.Linear(n_neurons, n_outputs)]\n",
    "    model = nn.Sequential(*layers)\n",
    "    model.apply(use_lecun_init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b175914",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "model = build_deep_model_with_selu(n_hidden=20, n_neurons=100, n_inputs=3*32*32, n_outputs=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fece6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.125)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = torchmetrics.Accuracy(task = \"multiclass\", num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "167235ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3894,  0.6269,  0.8503,  ..., -0.0957, -1.2816, -1.7623],\n",
       "        [ 0.3867, -0.7577, -0.0635,  ...,  0.6493, -1.4471, -1.2958],\n",
       "        [ 0.4182,  0.6508, -1.2004,  ...,  0.5712,  0.1454, -1.2473],\n",
       "        ...,\n",
       "        [-0.4236, -1.1303,  1.2127,  ...,  0.7121,  0.3489, -0.5387],\n",
       "        [-0.4858,  0.2503, -0.1575,  ...,  0.0178, -1.1537,  0.2549],\n",
       "        [-1.1143, -0.2282,  0.9924,  ...,  1.2536,  0.7671,  0.7227]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new, y_new = next(iter(train_loader))\n",
    "y_pred = model(X_new.to(device))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a7d717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_new.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cea203e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.716379165649414"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(y_pred, y_new)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6c2fbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d578685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: nan Train Metrics: 0.1004 Valid Metrics: 0.1014 (best) in 5.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m n_epochs = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs, patience, checkpoint_path, scheduler)\u001b[39m\n\u001b[32m     16\u001b[39m model.train()\n\u001b[32m     17\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/datasets/cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py:52\u001b[39m, in \u001b[36mCompose.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m     50\u001b[39m needs_unpacking = \u001b[38;5;28mlen\u001b[39m(inputs) > \u001b[32m1\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     outputs = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     inputs = outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:69\u001b[39m, in \u001b[36mTransform.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m     63\u001b[39m needs_transform_list = \u001b[38;5;28mself\u001b[39m._needs_transform_list(flat_inputs)\n\u001b[32m     64\u001b[39m params = \u001b[38;5;28mself\u001b[39m.make_params(\n\u001b[32m     65\u001b[39m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m flat_outputs = [\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[32m     71\u001b[39m ]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/transforms/v2/_type_conversion.py:39\u001b[39m, in \u001b[36mToImage.transform\u001b[39m\u001b[34m(self, inpt, params)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m, inpt: Union[torch.Tensor, PIL.Image.Image, np.ndarray], params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[32m     38\u001b[39m ) -> tv_tensors.Image:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_type_conversion.py:16\u001b[39m, in \u001b[36mto_image\u001b[39m\u001b[34m(inpt)\u001b[39m\n\u001b[32m     14\u001b[39m     output = torch.from_numpy(np.atleast_3d(inpt)).permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, PIL.Image.Image):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     output = \u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, torch.Tensor):\n\u001b[32m     18\u001b[39m     output = inpt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/torchvision/transforms/functional.py:209\u001b[39m, in \u001b[36mpil_to_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.as_tensor(nppic)\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m img = torch.as_tensor(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    210\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/PIL/Image.py:736\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    735\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes()\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43m_conv_type_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hands-on-ML/env/lib/python3.12/site-packages/PIL/Image.py:244\u001b[39m, in \u001b[36m_conv_type_shape\u001b[39m\u001b[34m(im)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Modes\u001b[39;00m\n\u001b[32m    241\u001b[39m _ENDIAN = \u001b[33m\"\u001b[39m\u001b[33m<\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conv_type_shape\u001b[39m(im: Image) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...], \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    245\u001b[39m     m = ImageMode.getmode(im.mode)\n\u001b[32m    246\u001b[39m     shape: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...] = (im.height, im.width)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "history = train_model(model, optimizer, loss_fn, accuracy, train_loader,\n",
    "                      valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0add23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_model_with_alpha_dropout(n_hidden, n_neurons, n_inputs, n_outputs, dropout_rate):\n",
    "    layers = [nn.Flatten(), standardize, nn.Linear(n_inputs, n_neurons), \n",
    "              nn.SELU(), nn.AlphaDropout(dropout_rate)]\n",
    "    for _ in range(n_hidden - 1):\n",
    "        layers += [nn.Linear(n_neurons, n_neurons), \n",
    "                   nn.SELU(), nn.AlphaDropout(dropout_rate)]\n",
    "    layers += [nn.Linear(n_neurons, n_outputs)]\n",
    "    model = nn.Sequential(*layers)\n",
    "    model.apply(use_lecun_init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c1905d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train Loss: 2.15371 Train Metrics: 0.1957 Valid Metrics: 0.2046 (best) in 5.5s\n",
      "Epoch: 2/100 Train Loss: 1.96613 Train Metrics: 0.2520 Valid Metrics: 0.2162 (best) in 5.7s\n",
      "Epoch: 3/100 Train Loss: 1.88170 Train Metrics: 0.2818 Valid Metrics: 0.3136 (best) in 4.9s\n",
      "Epoch: 4/100 Train Loss: 1.82748 Train Metrics: 0.3151 Valid Metrics: 0.3332 (best) in 5.0s\n",
      "Epoch: 5/100 Train Loss: 1.78960 Train Metrics: 0.3313 Valid Metrics: 0.3328 in 5.2s\n",
      "Epoch: 6/100 Train Loss: 1.73935 Train Metrics: 0.3548 Valid Metrics: 0.3602 (best) in 4.8s\n",
      "Epoch: 7/100 Train Loss: 1.70990 Train Metrics: 0.3702 Valid Metrics: 0.3586 in 4.7s\n",
      "Epoch: 8/100 Train Loss: 1.67694 Train Metrics: 0.3869 Valid Metrics: 0.3558 in 4.9s\n",
      "Epoch: 9/100 Train Loss: 1.65503 Train Metrics: 0.3947 Valid Metrics: 0.3862 (best) in 5.2s\n",
      "Epoch: 10/100 Train Loss: 1.63925 Train Metrics: 0.4026 Valid Metrics: 0.3782 in 5.5s\n",
      "Epoch: 11/100 Train Loss: 1.61455 Train Metrics: 0.4130 Valid Metrics: 0.3974 (best) in 6.2s\n",
      "Epoch: 12/100 Train Loss: 1.59450 Train Metrics: 0.4239 Valid Metrics: 0.4008 (best) in 5.6s\n",
      "Epoch: 13/100 Train Loss: 1.58717 Train Metrics: 0.4231 Valid Metrics: 0.4090 (best) in 6.1s\n",
      "Epoch: 14/100 Train Loss: 1.57252 Train Metrics: 0.4296 Valid Metrics: 0.3940 in 5.7s\n",
      "Epoch: 15/100 Train Loss: 1.57191 Train Metrics: 0.4330 Valid Metrics: 0.4056 in 5.3s\n",
      "Epoch: 16/100 Train Loss: 1.54523 Train Metrics: 0.4478 Valid Metrics: 0.4134 (best) in 5.3s\n",
      "Epoch: 17/100 Train Loss: 1.53823 Train Metrics: 0.4473 Valid Metrics: 0.4074 in 5.4s\n",
      "Epoch: 18/100 Train Loss: 1.51614 Train Metrics: 0.4572 Valid Metrics: 0.3972 in 5.2s\n",
      "Epoch: 19/100 Train Loss: 1.51176 Train Metrics: 0.4619 Valid Metrics: 0.4210 (best) in 5.2s\n",
      "Epoch: 20/100 Train Loss: 1.50995 Train Metrics: 0.4608 Valid Metrics: 0.4140 in 5.2s\n",
      "Epoch: 21/100 Train Loss: 1.52744 Train Metrics: 0.4540 Valid Metrics: 0.3884 in 5.3s\n",
      "Epoch: 22/100 Train Loss: 1.52051 Train Metrics: 0.4591 Valid Metrics: 0.4208 in 5.2s\n",
      "Epoch: 23/100 Train Loss: 1.48791 Train Metrics: 0.4728 Valid Metrics: 0.4166 in 5.2s\n",
      "Epoch: 24/100 Train Loss: 1.46968 Train Metrics: 0.4793 Valid Metrics: 0.4194 in 5.3s\n",
      "Epoch: 25/100 Train Loss: 1.46571 Train Metrics: 0.4814 Valid Metrics: 0.4212 (best) in 5.3s\n",
      "Epoch: 26/100 Train Loss: 1.47097 Train Metrics: 0.4845 Valid Metrics: 0.4194 in 5.2s\n",
      "Epoch: 27/100 Train Loss: 1.46562 Train Metrics: 0.4831 Valid Metrics: 0.4300 (best) in 5.4s\n",
      "Epoch: 28/100 Train Loss: 1.43888 Train Metrics: 0.4925 Valid Metrics: 0.4210 in 5.3s\n",
      "Epoch: 29/100 Train Loss: 1.43758 Train Metrics: 0.4931 Valid Metrics: 0.4304 (best) in 5.2s\n",
      "Epoch: 30/100 Train Loss: 1.43954 Train Metrics: 0.4919 Valid Metrics: 0.4184 in 5.3s\n",
      "Epoch: 31/100 Train Loss: 1.42485 Train Metrics: 0.4970 Valid Metrics: 0.4258 in 5.3s\n",
      "Epoch: 32/100 Train Loss: 1.42185 Train Metrics: 0.4984 Valid Metrics: 0.4316 (best) in 5.3s\n",
      "Epoch: 33/100 Train Loss: 1.42313 Train Metrics: 0.4990 Valid Metrics: 0.4254 in 5.4s\n",
      "Epoch: 34/100 Train Loss: 1.41667 Train Metrics: 0.5026 Valid Metrics: 0.4380 (best) in 5.3s\n",
      "Epoch: 35/100 Train Loss: 1.40964 Train Metrics: 0.5087 Valid Metrics: 0.4420 (best) in 5.3s\n",
      "Epoch: 36/100 Train Loss: 1.40133 Train Metrics: 0.5128 Valid Metrics: 0.4360 in 5.8s\n",
      "Epoch: 37/100 Train Loss: 1.40227 Train Metrics: 0.5077 Valid Metrics: 0.4402 in 5.6s\n",
      "Epoch: 38/100 Train Loss: 1.39529 Train Metrics: 0.5127 Valid Metrics: 0.4364 in 5.2s\n",
      "Epoch: 39/100 Train Loss: 1.38885 Train Metrics: 0.5156 Valid Metrics: 0.4526 (best) in 5.2s\n",
      "Epoch: 40/100 Train Loss: 1.39075 Train Metrics: 0.5128 Valid Metrics: 0.4400 in 5.5s\n",
      "Epoch: 41/100 Train Loss: 1.39319 Train Metrics: 0.5128 Valid Metrics: 0.4410 in 5.2s\n",
      "Epoch: 42/100 Train Loss: 1.37947 Train Metrics: 0.5183 Valid Metrics: 0.4430 in 5.2s\n",
      "Epoch: 43/100 Train Loss: 1.40080 Train Metrics: 0.5105 Valid Metrics: 0.4230 in 5.3s\n",
      "Epoch: 44/100 Train Loss: 1.39365 Train Metrics: 0.5133 Valid Metrics: 0.4336 in 5.2s\n",
      "Epoch: 45/100 Train Loss: 1.37188 Train Metrics: 0.5222 Valid Metrics: 0.4302 in 5.2s\n",
      "Epoch: 46/100 Train Loss: 1.37281 Train Metrics: 0.5220 Valid Metrics: 0.4172 in 5.4s\n",
      "Epoch: 47/100 Train Loss: 1.39839 Train Metrics: 0.5124 Valid Metrics: 0.4338 in 5.3s\n",
      "Epoch: 48/100 Train Loss: 1.37193 Train Metrics: 0.5194 Valid Metrics: 0.4430 in 5.2s\n",
      "Epoch: 49/100 Train Loss: 1.36101 Train Metrics: 0.5255 Valid Metrics: 0.4370 in 5.2s\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = build_deep_model_with_alpha_dropout(n_hidden=20, n_neurons=100, n_inputs=3*32*32, n_outputs=10, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr = 1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = torchmetrics.Accuracy(task = \"multiclass\", num_classes=10).to(device)\n",
    "n_epochs = 100\n",
    "history = train_model(model, optimizer, loss_fn, accuracy, train_loader, valid_loader, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e65e64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/60 Train Loss: 2.26607 Train Metrics: 0.1722 Valid Metrics: 0.2160 (best) in 5.3s\n",
      "Epoch: 2/60 Train Loss: 2.01805 Train Metrics: 0.2334 Valid Metrics: 0.2520 (best) in 5.1s\n",
      "Epoch: 3/60 Train Loss: 1.95107 Train Metrics: 0.2530 Valid Metrics: 0.2716 (best) in 5.4s\n",
      "Epoch: 4/60 Train Loss: 1.91026 Train Metrics: 0.2703 Valid Metrics: 0.2926 (best) in 5.0s\n",
      "Epoch: 5/60 Train Loss: 1.87044 Train Metrics: 0.2857 Valid Metrics: 0.3044 (best) in 4.9s\n",
      "Epoch: 6/60 Train Loss: 1.83257 Train Metrics: 0.3069 Valid Metrics: 0.3232 (best) in 4.8s\n",
      "Epoch: 7/60 Train Loss: 1.80264 Train Metrics: 0.3244 Valid Metrics: 0.3440 (best) in 4.8s\n",
      "Epoch: 8/60 Train Loss: 1.76939 Train Metrics: 0.3379 Valid Metrics: 0.3254 in 4.8s\n",
      "Epoch: 9/60 Train Loss: 1.73349 Train Metrics: 0.3514 Valid Metrics: 0.3600 (best) in 4.9s\n",
      "Epoch: 10/60 Train Loss: 1.70743 Train Metrics: 0.3666 Valid Metrics: 0.3422 in 5.2s\n",
      "Epoch: 11/60 Train Loss: 1.67766 Train Metrics: 0.3800 Valid Metrics: 0.3676 (best) in 5.4s\n",
      "Epoch: 12/60 Train Loss: 1.65078 Train Metrics: 0.3878 Valid Metrics: 0.3762 (best) in 5.3s\n",
      "Epoch: 13/60 Train Loss: 1.62723 Train Metrics: 0.3992 Valid Metrics: 0.3798 (best) in 5.3s\n",
      "Epoch: 14/60 Train Loss: 1.60837 Train Metrics: 0.4045 Valid Metrics: 0.3914 (best) in 5.4s\n",
      "Epoch: 15/60 Train Loss: 1.58580 Train Metrics: 0.4173 Valid Metrics: 0.3862 in 5.2s\n",
      "Epoch: 16/60 Train Loss: 1.56703 Train Metrics: 0.4267 Valid Metrics: 0.3926 (best) in 5.3s\n",
      "Epoch: 17/60 Train Loss: 1.55250 Train Metrics: 0.4344 Valid Metrics: 0.3896 in 5.4s\n",
      "Epoch: 18/60 Train Loss: 1.53355 Train Metrics: 0.4407 Valid Metrics: 0.4066 (best) in 5.3s\n",
      "Epoch: 19/60 Train Loss: 1.52045 Train Metrics: 0.4453 Valid Metrics: 0.3986 in 5.2s\n",
      "Epoch: 20/60 Train Loss: 1.50929 Train Metrics: 0.4504 Valid Metrics: 0.3996 in 5.3s\n",
      "Epoch: 21/60 Train Loss: 1.49267 Train Metrics: 0.4544 Valid Metrics: 0.4046 in 5.3s\n",
      "Epoch: 22/60 Train Loss: 1.47388 Train Metrics: 0.4646 Valid Metrics: 0.4110 (best) in 5.3s\n",
      "Epoch: 23/60 Train Loss: 1.46860 Train Metrics: 0.4668 Valid Metrics: 0.4142 (best) in 5.4s\n",
      "Epoch: 24/60 Train Loss: 1.45869 Train Metrics: 0.4710 Valid Metrics: 0.4060 in 5.3s\n",
      "Epoch: 25/60 Train Loss: 1.44384 Train Metrics: 0.4730 Valid Metrics: 0.4110 in 5.3s\n",
      "Epoch: 26/60 Train Loss: 1.43620 Train Metrics: 0.4796 Valid Metrics: 0.4262 (best) in 5.3s\n",
      "Epoch: 27/60 Train Loss: 1.42630 Train Metrics: 0.4827 Valid Metrics: 0.4180 in 5.2s\n",
      "Epoch: 28/60 Train Loss: 1.41203 Train Metrics: 0.4887 Valid Metrics: 0.4162 in 5.2s\n",
      "Epoch: 29/60 Train Loss: 1.41070 Train Metrics: 0.4900 Valid Metrics: 0.4296 (best) in 5.4s\n",
      "Epoch: 30/60 Train Loss: 1.40348 Train Metrics: 0.4962 Valid Metrics: 0.4238 in 5.5s\n",
      "Epoch: 31/60 Train Loss: 1.38840 Train Metrics: 0.5007 Valid Metrics: 0.4262 in 5.7s\n",
      "Epoch: 32/60 Train Loss: 1.38483 Train Metrics: 0.5040 Valid Metrics: 0.4264 in 5.5s\n",
      "Epoch: 33/60 Train Loss: 1.37526 Train Metrics: 0.5068 Valid Metrics: 0.4250 in 5.2s\n",
      "Epoch: 34/60 Train Loss: 1.36696 Train Metrics: 0.5102 Valid Metrics: 0.4292 in 5.2s\n",
      "Epoch: 35/60 Train Loss: 1.35913 Train Metrics: 0.5146 Valid Metrics: 0.4192 in 5.1s\n",
      "Epoch: 36/60 Train Loss: 1.34965 Train Metrics: 0.5186 Valid Metrics: 0.4390 (best) in 5.4s\n",
      "Epoch: 37/60 Train Loss: 1.35015 Train Metrics: 0.5159 Valid Metrics: 0.4250 in 5.3s\n",
      "Epoch: 38/60 Train Loss: 1.34201 Train Metrics: 0.5216 Valid Metrics: 0.4316 in 5.3s\n",
      "Epoch: 39/60 Train Loss: 1.33018 Train Metrics: 0.5268 Valid Metrics: 0.4190 in 5.1s\n",
      "Epoch: 40/60 Train Loss: 1.32102 Train Metrics: 0.5304 Valid Metrics: 0.4312 in 5.3s\n",
      "Epoch: 41/60 Train Loss: 1.31586 Train Metrics: 0.5319 Valid Metrics: 0.4186 in 5.3s\n",
      "Epoch: 42/60 Train Loss: 1.31542 Train Metrics: 0.5334 Valid Metrics: 0.4344 in 5.4s\n",
      "Epoch: 43/60 Train Loss: 1.31116 Train Metrics: 0.5340 Valid Metrics: 0.4352 in 5.1s\n",
      "Epoch: 44/60 Train Loss: 1.29655 Train Metrics: 0.5400 Valid Metrics: 0.4420 (best) in 5.2s\n",
      "Epoch: 45/60 Train Loss: 1.29765 Train Metrics: 0.5386 Valid Metrics: 0.4242 in 5.2s\n",
      "Epoch: 46/60 Train Loss: 1.29804 Train Metrics: 0.5400 Valid Metrics: 0.4342 in 5.2s\n",
      "Epoch: 47/60 Train Loss: 1.28830 Train Metrics: 0.5425 Valid Metrics: 0.4422 (best) in 5.2s\n",
      "Epoch: 48/60 Train Loss: 1.27595 Train Metrics: 0.5482 Valid Metrics: 0.4306 in 5.2s\n",
      "Epoch: 49/60 Train Loss: 1.27823 Train Metrics: 0.5503 Valid Metrics: 0.4450 (best) in 5.3s\n",
      "Epoch: 50/60 Train Loss: 1.26510 Train Metrics: 0.5536 Valid Metrics: 0.4410 in 5.2s\n",
      "Epoch: 51/60 Train Loss: 1.26433 Train Metrics: 0.5558 Valid Metrics: 0.4372 in 5.2s\n",
      "Epoch: 52/60 Train Loss: 1.26564 Train Metrics: 0.5508 Valid Metrics: 0.4342 in 5.1s\n",
      "Epoch: 53/60 Train Loss: 1.26127 Train Metrics: 0.5548 Valid Metrics: 0.4488 (best) in 5.2s\n",
      "Epoch: 54/60 Train Loss: 1.25353 Train Metrics: 0.5583 Valid Metrics: 0.4394 in 5.3s\n",
      "Epoch: 55/60 Train Loss: 1.24894 Train Metrics: 0.5589 Valid Metrics: 0.4504 (best) in 5.3s\n",
      "Epoch: 56/60 Train Loss: 1.24384 Train Metrics: 0.5629 Valid Metrics: 0.4518 (best) in 5.2s\n",
      "Epoch: 57/60 Train Loss: 1.23584 Train Metrics: 0.5646 Valid Metrics: 0.4294 in 5.2s\n",
      "Epoch: 58/60 Train Loss: 1.23885 Train Metrics: 0.5620 Valid Metrics: 0.4426 in 5.3s\n",
      "Epoch: 59/60 Train Loss: 1.22869 Train Metrics: 0.5653 Valid Metrics: 0.4394 in 5.2s\n",
      "Epoch: 60/60 Train Loss: 1.21966 Train Metrics: 0.5685 Valid Metrics: 0.4426 in 5.2s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n_epochs = 60\n",
    "model = build_deep_model_with_alpha_dropout(n_hidden=20, n_neurons=100, n_inputs=3*32*32, n_outputs=10, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 1e-2, epochs=n_epochs, steps_per_epoch=len(train_loader))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = torchmetrics.Accuracy(task = \"multiclass\", num_classes=10).to(device)\n",
    "\n",
    "history = train_model(model, optimizer, loss_fn, accuracy, train_loader,\n",
    "                      valid_loader, n_epochs, patience=20, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
